{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <tr><td>\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a></td><td>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.</td>\n",
    "    </tr>\n",
    "    <tr><td>Jacques Roy, Byte Size Data Science</td><td> </td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding and Preparation (part 2)\n",
    "To understand data, we need to explore it.\n",
    "\n",
    "This adds to the following videos:\n",
    "- <a href=\"https://youtu.be/xSDP6u_Xqhc\">017-Spark Data Exploration</a>\n",
    "- <a href=\"https://youtu.be/AeeHapnLhyE\">018-Python Pandas Data Exploration</a>\n",
    "- <a href=\"https://youtu.be/qw4FtewQFZE\">032-JDBC Data Exploration</a>\n",
    "- <a href=\"https://youtu.be/qw4FtewQFZE\">060-Data Understanding and Preparation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 060-Data Understanding and Preparation\n",
    "Execute the next cell if you want to see the `Byte Size Data Science` youtube channel video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/wJQ-5Cm1H0E?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the appropriate libraries and set up needed connections\n",
    "There is another library to connect to db2. See: https://pythonhosted.org/ibmdbpy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ibm_db\n",
    "import ibm_db_dbi\n",
    "import math\n",
    "\n",
    "from ftplib import FTP\n",
    "import requests, zipfile, io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    'username': 'bluadmin',\n",
    "    'password': \"\"\"PASSWORD\"\"\",\n",
    "    'sg_service_url': 'https://sgmanager.ng.bluemix.net',\n",
    "    'database': 'BLUDB',\n",
    "    'host': 'dashdb-enterprise-. . . .bluemix.net',\n",
    "    'port': '50001',\n",
    "    'url': 'https://undefined'\n",
    "}\n",
    "schema=\"CHICAGO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn = (\n",
    "    \"DRIVER={{IBM DB2 ODBC DRIVER}};\"\n",
    "    \"DATABASE={0};\"\n",
    "    \"HOSTNAME={1};\"\n",
    "    \"PORT={2};\"\n",
    "    \"PROTOCOL=TCPIP;\"\n",
    "    \"SECURITY=ssl;\" \n",
    "    \"UID={3};\"\n",
    "    \"PWD={4};\").format(credentials['database'], credentials['host'],\n",
    "                       credentials['port'], credentials['username'],\n",
    "                       credentials['password'])\n",
    "\n",
    "conn = ibm_db.connect(dsn, \"\", \"\")\n",
    "pconn = ibm_db_dbi.Connection(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns that we want to convert into separate tables\n",
    "cat_columns = ['TRAFFIC_CONTROL_DEVICE','DEVICE_CONDITION','WEATHER_CONDITION','LIGHTING_CONDITION',\n",
    "           'FIRST_CRASH_TYPE','TRAFFICWAY_TYPE','ALIGNMENT','ROADWAY_SURFACE_COND','ROAD_DEFECT',\n",
    "           'REPORT_TYPE','CRASH_TYPE','DAMAGE','PRIM_CONTRIBUTORY_CAUSE','SEC_CONTRIBUTORY_CAUSE',\n",
    "           'WORK_ZONE_TYPE','MOST_SEVERE_INJURY'\n",
    "          ]\n",
    "other_cat_columns = ['POSTED_SPEED_LIMIT','LANE_CNT','NUM_UNITS', 'INJURIES_TOTAL',\n",
    "                     'CRASH_HOUR','CRASH_DAY_OF_WEEK','CRASH_MONTH']\n",
    "cat_all = cat_columns + other_cat_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the distribution of each categorical attribute\n",
    "### Build the SQL statement\n",
    "We need to get all the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing but with Quries to the table.\n",
    "# I have to build a series of SQL statements and do a UNION ALL on them\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT '{1}' COLNAME, attr.id COLVALUE, COUNT(*) VALCOUNT\n",
    "FROM {0}.staging_ChicagoAccidents acc, {0}.{1}_table attr\n",
    "WHERE acc.{1} = attr.description\n",
    "GROUP BY '{1}', attr.id \n",
    "\"\"\"\n",
    "\n",
    "query2 = \"\"\"\n",
    "SELECT '{1}' COLNAME, {1} COLVALUE, COUNT(*) VALCOUNT\n",
    "FROM {0}.staging_ChicagoAccidents acc\n",
    "GROUP BY '{1}', {1}\n",
    "\"\"\"\n",
    "\n",
    "sql = \"\"\n",
    "for name in cat_columns :\n",
    "    if (len(sql) > 0 ) :\n",
    "        sql = sql + \"UNION ALL\"\n",
    "    sql = sql + query.format(schema,name)\n",
    "for name in other_cat_columns :\n",
    "    sql = sql + \"UNION ALL\"\n",
    "    sql = sql + query2.format(schema,name)\n",
    "\n",
    "stats_pd = pd.read_sql(sql, pconn)\n",
    "print(\"Number of records: {0}\".format(stats_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_rows = math.ceil(len(cat_all) / 2)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nb_rows, ncols=2)\n",
    "fig.set_figheight(75)\n",
    "fig.set_figwidth(15)\n",
    "for ix, ax in enumerate(axes.flatten()) :\n",
    "    if (ix < len(cat_all) ) :\n",
    "        tmp_pd = stats_pd[stats_pd['COLNAME'] == cat_all[ix]].sort_values(by=['COLVALUE'])\n",
    "        tmp_pd.plot.bar(ax=ax, x='COLVALUE', y='VALCOUNT',title=cat_all[ix], legend=False)\n",
    "        ax.set_xlabel('')\n",
    "    else:\n",
    "        fig.delaxes(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping categories\n",
    "We make the following (arbitrary) decisions:\n",
    "- TRAFFIC_CONTROL_DEVICE: create 3 groups\n",
    "- DEVICE_CONDITION: create 3 groups\n",
    "- WEATHER_CONDITION: create 2 groups\n",
    "- LIGHTING_CONDITION: create 2 groups\n",
    "- FIRST_CRASH_TYPE: create 6 groups\n",
    "- TRAFFICWAY_TYPE: create 2 groups\n",
    "- ALIGNMENT:ignore this attribute\n",
    "- ROADWAY_SURFACE_COND: create 2 groups\n",
    "- ROAD_DEFECT: Create 2 groups\n",
    "- REPORT_TYPE: keep as-is\n",
    "- CRASH_TYPE:keep as-is\n",
    "- DAMAGE: keep as-is\n",
    "- PRIM_CONTRIBUTORY_CAUSE: create 4 groups\n",
    "- SEC_CONTRIBUTORY_CAUSE: create 3 groups\n",
    "- WORK_ZONE_TYPE: ignore this attribute\n",
    "- MOST_SEVERE_INJURY: ignore this attribute\n",
    "- POSTED_SPEED_LIMIT: create 2 groups (30 or other)\n",
    "- LANE_CNT: create 3 groups (2, 4, other)\n",
    "- NUM_UNITS: ignore this attribute\n",
    "- INJURIES_TOTAL: ignore this attribute\n",
    "- CRASH_HOUR: keep as-is\n",
    "- CRASH_DAY_OF_WEEK: keep as-is\n",
    "- CRASH_MONTH: keep as-is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the tables to add a grouping column\n",
    "For the attributes that already have \"side\" tables created, we need to modify them to add a column.<br/>\n",
    "Then we need to populate them according to our decisions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_def = \"\"\"\n",
    "ALTER TABLE {0}.{1}_TABLE ADD COLUMN grouping INTEGER DEFAULT 0\n",
    "\"\"\"\n",
    "\n",
    "for col in cat_columns :\n",
    "    sql = alter_def.format(schema,col)\n",
    "    cur = pconn.cursor()\n",
    "    cur.execute(sql)\n",
    "    print(\"Table {0}_TABLE altered\".format(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tables for the other columns\n",
    "Add the grouping column right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_def = \"\"\"\n",
    "CREATE TABLE {0}.{1}_TABLE (\n",
    "    id          INT NOT NULL,\n",
    "    description INT,\n",
    "    grouping    INT default 0,\n",
    "\n",
    "    PRIMARY KEY(id)\n",
    ") ORGANIZE BY ROW;\n",
    "\"\"\"\n",
    "\n",
    "for col in other_cat_columns :\n",
    "    sql = table_def.format(schema,col)\n",
    "    cur = pconn.cursor()\n",
    "    cur.execute(sql)\n",
    "    print(\"Table {0}_TABLE created\".format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the tables\n",
    "insert_sql = \"\"\"\n",
    "  INSERT INTO {0}.{1}_TABLE(id,description)\n",
    "  SELECT {1}, {1}\n",
    "  FROM (\n",
    "     SELECT distinct {1} \n",
    "     FROM {0}.staging_ChicagoAccidents\n",
    "  )\n",
    "\"\"\"\n",
    "for col in other_cat_columns :\n",
    "    sql = insert_sql.format(schema,col)\n",
    "    cur = pconn.cursor()\n",
    "    cur.execute(sql)\n",
    "    print(\"Table {0}_TABLE populated\".format(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement our grouping\n",
    "We created the tables with a default value for grouping. That default value would be for \"other\".<br/>\n",
    "This way, we only need to update the exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two types of SQL statements we need to execute\n",
    "sql = \"\"\"\n",
    "UPDATE {0}.{1}_TABLE\n",
    "SET grouping = {3}\n",
    "WHERE id = {2}\n",
    "\"\"\"\n",
    "sql2 = \"\"\"\n",
    "UPDATE {0}.{1}_TABLE\n",
    "SET grouping = id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAFFIC_CONTROL_DEVICE: 15 categories. 12-group1, 13-group2, all others group3\n",
    "d = {'id': [12,13], 'grouping': list(range(1,3))}\n",
    "cur = pconn.cursor()\n",
    "for ix in range(len(d['id'])) :\n",
    "    cur.execute(sql.format(schema,'TRAFFIC_CONTROL_DEVICE',d['id'][ix],d['grouping'][ix]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEATHER_CONDITION\n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql.format(schema,'WEATHER_CONDITION',8,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE_CONDITION: 8 categories, define 6, 7\n",
    "d = {'id': [6,7], 'grouping': [1,2]}\n",
    "cur = pconn.cursor()\n",
    "for ix in range(len(d['id'])) :\n",
    "    cur.execute(sql.format(schema,'DEVICE_CONDITION',d['id'][ix],d['grouping'][ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTING_CONDITION\n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql.format(schema,'LIGHTING_CONDITION',3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST_CRASH_TYPE\n",
    "d = {'id': [4,7,13,14,15], 'grouping': [1,2,3,4,5]}\n",
    "cur = pconn.cursor()\n",
    "for ix in range(len(d['id'])) :\n",
    "    cur.execute(sql.format(schema,'FIRST_CRASH_TYPE',d['id'][ix],d['grouping'][ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAFFICWAY_TYPE: create 2 groups \n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql.format(schema,'TRAFFICWAY_TYPE',9,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIGNMENT:ignore this attribute\n",
    "# ROADWAY_SURFACE_COND: create 2 groups\n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql.format(schema,'ROADWAY_SURFACE_COND',2,1))\n",
    "# ROAD_DEFECT: Create 2 groups\n",
    "result = cur.execute(sql.format(schema,'ROAD_DEFECT',6,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPORT_TYPE: keep as-is\n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql2.format(schema,'REPORT_TYPE'))\n",
    "# CRASH_TYPE:keep as-is\n",
    "result = cur.execute(sql2.format(schema,'CRASH_TYPE'))\n",
    "# DAMAGE: keep as-is\n",
    "result = cur.execute(sql2.format(schema,'DAMAGE'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIM_CONTRIBUTORY_CAUSE: create 4 groups\n",
    "d = {'id': [8,11,22], 'grouping': [1,2,3]}\n",
    "cur = pconn.cursor()\n",
    "for ix in range(len(d['id'])) :\n",
    "    cur.execute(sql.format(schema,'PRIM_CONTRIBUTORY_CAUSE',d['id'][ix],d['grouping'][ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEC_CONTRIBUTORY_CAUSE: create 3 groups\n",
    "d = {'id': [30,38], 'grouping': [1,2]}\n",
    "cur = pconn.cursor()\n",
    "for ix in range(len(d['id'])) :\n",
    "    cur.execute(sql.format(schema,'SEC_CONTRIBUTORY_CAUSE',d['id'][ix],d['grouping'][ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK_ZONE_TYPE: ignore this attribute\n",
    "# MOST_SEVERE_INJURY: ignore this attribute\n",
    "# POSTED_SPEED_LIMIT: create 2 groups (30 or other)\n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql.format(schema,'POSTED_SPEED_LIMIT',30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANE_CNT: create 3 groups (2, 4, other)\n",
    "d = {'id': [2,4], 'grouping': [1,2]}\n",
    "cur = pconn.cursor()\n",
    "for ix in range(len(d['id'])) :\n",
    "    cur.execute(sql.format(schema,'LANE_CNT',d['id'][ix],d['grouping'][ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_UNITS: ignore this attribute\n",
    "# INJURIES_TOTAL\n",
    "# CRASH_HOUR: keep as-is\n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql2.format(schema,'CRASH_HOUR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRASH_DAY_OF_WEEK: keep as-is\n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql2.format(schema,'CRASH_DAY_OF_WEEK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRASH_MONTH: keep as-is\n",
    "cur = pconn.cursor()\n",
    "result = cur.execute(sql2.format(schema,'CRASH_MONTH'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "We can also get an idea of how numerical attributes relate to each other.\n",
    "\n",
    "We could read all the records into a Pandas dataframe and then do the correlation.\n",
    "Instead, we use a different interface to the db2 table, **`ibmdbpy`**, to get a reference\n",
    "to the table and have the correlation done in the database server.\n",
    "\n",
    "This is a lot faster and efficient than reading all the data and then do the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibmdbpy import IdaDataBase, IdaDataFrame\n",
    "\n",
    "dsn2 = 'DASHDB;Database={0};Hostname={1};Port={2};PROTOCOL=TCPIP;SECURITY=ssl;UID={3};PWD={4}'.\\\n",
    "format(credentials['database'], credentials['host'],\n",
    "                       credentials['port'], credentials['username'],\n",
    "                       credentials['password'])\n",
    "idadb = IdaDataBase(dsn=dsn2)\n",
    "ida_df = IdaDataFrame(idadb, '{0}.STAGING_CHICAGOACCIDENTS'.format(schema))\n",
    "corr_pd = ida_df.corr()\n",
    "# idadb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f = plt.figure(figsize=(12, 12))\n",
    "plt.matshow(corr_pd, fignum=f.number)\n",
    "plt.xticks(range(corr_pd.shape[1]), corr_pd.columns, fontsize=8, rotation=45)\n",
    "plt.yticks(range(corr_pd.shape[1]), corr_pd.columns, fontsize=8)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the final table\n",
    "First, go get the column definitions so we can use the proper data types.\n",
    "\n",
    "The column type seems to be followed by a space, we need to accomodate for that. Also, the TIMESTAMP type is listed as TIMESTMP so we need to cover this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT NAME,COLTYPE,LENGTH,SCALE, NULLS\n",
    "FROM SYSIBM.SYSCOLUMNS\n",
    "WHERE TBNAME = 'STAGING_CHICAGOACCIDENTS'\n",
    "AND   TBCREATOR = '{0}'\n",
    "ORDER BY COLNO;\n",
    "\"\"\".format(schema)\n",
    "tabdef_pd = pd.read_sql(sql, pconn)\n",
    "tabdef_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table. This is just helper code where I added the definition of the primary key\n",
    "\n",
    "column_types = {\"VARCHAR\": \"VARCHAR\", \"CHAR\": \"CHAR\", \"INTEGER\":\"INTEGER\",\"DOUBLE\":\"DOUBLE\", \"TIMESTMP\": \"TIMESTAMP\"}\n",
    "\n",
    "sql = \"CREATE TABLE {0}.ChicagoAccidents (\\n\".format(schema)\n",
    "\n",
    "for row in tabdef_pd.iterrows() :\n",
    "    if row[1]['NAME'] in cat_columns :\n",
    "        sql = sql + \"  {0:33} INTEGER REFERENCES {1}.{2}_TABLE(ID),\\n\".format(row[1]['NAME'] + \"_ID\",schema,row[1]['NAME'])\n",
    "    else :\n",
    "        # we need to add the type, length, and if not null\n",
    "        sql = sql + \"  {0:33} {1}\".format(row[1]['NAME'],column_types[row[1]['COLTYPE'].strip()] )\n",
    "        if row[1]['COLTYPE'].find('CHAR') > -1 :\n",
    "            sql = sql + \"({0}) \".format(row[1]['LENGTH'])\n",
    "        if row[1]['NULLS'] == 'N' :\n",
    "            sql = sql + \"NOT NULL \"\n",
    "        sql = sql + \",\\n\"\n",
    "\n",
    "sql = sql + \"\\n  PRIMARY KEY(RD_NO)\\n) ORGANIZE BY ROW;\"\n",
    "# print(sql)\n",
    "cur = pconn.cursor()\n",
    "cur.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the new table\n",
    "We build the SQL statement programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"INSERT INTO {0}.ChicagoAccidents \\nSELECT \".format(schema)\n",
    "\n",
    "# Identify the columns\n",
    "for row in tabdef_pd.iterrows() :\n",
    "    if row[1]['NAME'] in cat_columns :\n",
    "        sql = sql + \"{0}.{1}_table.id as {1}_ID,\".format(schema,row[1]['NAME'])\n",
    "    else :\n",
    "        sql = sql + \"{0},\".format(row[1]['NAME'])\n",
    "\n",
    "sql = sql[:-1] + \"\\n FROM {0}.staging_ChicagoAccidents,\".format(schema)\n",
    "\n",
    "# Add the other tables\n",
    "for name in cat_columns :\n",
    "    sql = sql + \"{0}.{1}_table,\".format(schema,name)\n",
    "sql = sql[:-1] + \"\\n WHERE \"\n",
    "\n",
    "# Add the conditions\n",
    "for name in cat_columns :\n",
    "    sql = sql + \"{0}.staging_ChicagoAccidents.{1} = {0}.{1}_table.description\\nAND \".format(schema,name) \n",
    "sql = sql[:-4] + \";\"\n",
    "\n",
    "# print(sql)\n",
    "cur = pconn.cursor()\n",
    "cur.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
