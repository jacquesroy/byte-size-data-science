{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <tr><td>\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a></td><td>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.</td>\n",
    "    </tr>\n",
    "    <tr><td>Jacques Roy, Byte Size Data Science</td><td> </td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding / Modeling\n",
    "We are trying two types of modeling:\n",
    "- Hierarchical clustering\n",
    "- DBSCAN (Density-based spatial clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/NoZfnj4vbAg?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the appropriate libraries and set up needed connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ibm_db\n",
    "import ibm_db_dbi\n",
    "\n",
    "from ftplib import FTP\n",
    "import requests, zipfile, io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium 2>&1 >foliumpip.out\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    'username': 'bluadmin',\n",
    "    'password': \"\"\"PASSWORD\"\"\",\n",
    "    'sg_service_url': 'https://sgmanager.ng.bluemix.net',\n",
    "    'database': 'BLUDB',\n",
    "    'host': 'dashdb-. . .',\n",
    "    'port': '50001',\n",
    "    'url': 'https://undefined'\n",
    "}\n",
    "schema=\"CHICAGO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn = (\n",
    "    \"DRIVER={{IBM DB2 ODBC DRIVER}};\"\n",
    "    \"DATABASE={0};\"\n",
    "    \"HOSTNAME={1};\"\n",
    "    \"PORT={2};\"\n",
    "    \"PROTOCOL=TCPIP;\"\n",
    "    \"SECURITY=ssl;\"\n",
    "    \"UID={3};\"\n",
    "    \"PWD={4};\").format(credentials['database'], credentials['host'],\n",
    "                       credentials['port'], credentials['username'],\n",
    "                       credentials['password'])\n",
    "\n",
    "conn = ibm_db.connect(dsn, \"\", \"\")\n",
    "pconn = ibm_db_dbi.Connection(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago accident data\n",
    "We are using the Chicago accident date that we discussed in videos 59 and 60.\n",
    "\n",
    "In this notebook, we'll use a sampling subset of out entire dataset (157,852 rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical values distribution\n",
    "Please review video 60 to find out what was done for the categorical values.\n",
    "\n",
    "I should havew followed the <column_name>_ID naming convention for the numerical categorical attributes.\n",
    "That would have been more consistent and saved me some coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['TRAFFIC_CONTROL_DEVICE_ID','DEVICE_CONDITION_ID','WEATHER_CONDITION_ID','LIGHTING_CONDITION_ID',\n",
    "           'FIRST_CRASH_TYPE_ID','TRAFFICWAY_TYPE_ID','ALIGNMENT_ID','ROADWAY_SURFACE_COND_ID','ROAD_DEFECT_ID',\n",
    "           'REPORT_TYPE_ID','CRASH_TYPE_ID','DAMAGE_ID','PRIM_CONTRIBUTORY_CAUSE_ID','SEC_CONTRIBUTORY_CAUSE_ID',\n",
    "           'WORK_ZONE_TYPE_ID','MOST_SEVERE_INJURY_ID'\n",
    "          ]\n",
    "other_cat_columns = ['POSTED_SPEED_LIMIT','LANE_CNT','NUM_UNITS', 'INJURIES_TOTAL',\n",
    "                     'CRASH_HOUR','CRASH_DAY_OF_WEEK','CRASH_MONTH']\n",
    "\n",
    "cat_all = categorical_columns + other_cat_columns\n",
    "\n",
    "# Identify the columns we want to drop from the modeling\n",
    "dropped_columns = {'RD_NO','CRASH_DATE_EST_I','CRASH_DATE','POSTED_SPEED_LIMIT','DATE_POLICE_NOTIFIED','STREET_NO',\n",
    "                   'STREET_NAME'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "We need to build the SQL statement that accesses all the referenced tables.<br/>\n",
    "This applies the grouping we want for each categorical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick query to get the column names\n",
    "sql = \"\"\"\n",
    "SELECT NAME,COLTYPE,LENGTH,SCALE, NULLS\n",
    "FROM SYSIBM.SYSCOLUMNS\n",
    "WHERE TBNAME = 'CHICAGOACCIDENTS'\n",
    "AND   TBCREATOR = '{0}'\n",
    "ORDER BY COLNO;\n",
    "\"\"\".format(schema)\n",
    "tabdef_pd = pd.read_sql(sql, pconn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = \" TABLESAMPLE SYSTEM (2) \"\n",
    "# limitrows = \" LIMIT 2000 \"\n",
    "limitrows = \"\"\n",
    "sql = \"SELECT \"\n",
    "\n",
    "# Identify the columns\n",
    "for row in tabdef_pd.iterrows() :\n",
    "    if row[1]['NAME'] in dropped_columns : # skip the columns we don't want to use in modeling\n",
    "        continue\n",
    "    if row[1]['NAME'] in categorical_columns :\n",
    "        sql = sql + \"{0}.{1}_table.grouping as {1}_ID,\\n\".format(schema,row[1]['NAME'][:-3])\n",
    "    elif row[1]['NAME'] in other_cat_columns :\n",
    "        sql = sql + \"{0}.{1}_table.grouping as {1}_ID,\\n\".format(schema,row[1]['NAME'])\n",
    "    else :\n",
    "        sql = sql + row[1]['NAME'] + \",\"\n",
    "\n",
    "sql = sql[:-1] + \"\\n FROM {0}.ChicagoAccidents {1},\".format(schema,sampling)\n",
    "\n",
    "# Add the other tables\n",
    "for name in cat_all :\n",
    "    if name in dropped_columns : # skip the columns we don't want to use in modeling\n",
    "        continue\n",
    "    if name in other_cat_columns :\n",
    "        sql = sql + \"{0}.{1}_table,\".format(schema,name)\n",
    "    else:\n",
    "        sql = sql + \"{0}.{1}_table,\".format(schema,name[:-3])\n",
    "sql = sql[:-1] + \"\\n WHERE \"\n",
    "\n",
    "# Add the conditions\n",
    "for name in cat_all :\n",
    "    if name in dropped_columns : # skip the columns we don't want to use in modeling\n",
    "        continue\n",
    "    if name in other_cat_columns :\n",
    "        sql = sql + \"{0}.ChicagoAccidents.{1} = {0}.{1}_table.id\\nAND \".format(schema,name)\n",
    "    else:\n",
    "        sql = sql + \"{0}.ChicagoAccidents.{1}_id = {0}.{1}_table.id\\nAND \".format(schema,name[:-3]) \n",
    "\n",
    "sql = sql[:-4] + \" {0};\".format(limitrows)\n",
    "\n",
    "data_pd = pd.read_sql(sql, pconn)\n",
    "print(\"Number of records: {0}\".format(data_pd.shape[0]))\n",
    "data_pd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder #, OneHotEncoder\n",
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character columns to encode, \n",
    "char_columns = [\"INTERSECTION_RELATED_I\",\"NOT_RIGHT_OF_WAY_I\",\"HIT_AND_RUN_I\",\n",
    "                \"STREET_DIRECTION\",\"PHOTOS_TAKEN_I\",\"STATEMENTS_TAKEN_I\",\"DOORING_I\",\n",
    "                \"WORK_ZONE_I\",\"WORKERS_PRESENT_I\"]\n",
    "encoded_pd = data_pd.copy()\n",
    "for col in char_columns:\n",
    "    encoded_pd[col] = LabelEncoder().fit_transform(encoded_pd[col])\n",
    "encoded_pd = encoded_pd.drop([\"LATITUDE\",\"LONGITUDE\"],axis=1)\n",
    "encoded_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = normalize(encoded_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import objects assuming the k-means section was skipped\n",
    "from scipy import ndimage \n",
    "from scipy.cluster import hierarchy \n",
    "from scipy.spatial import distance_matrix \n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import manifold, datasets \n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn.datasets.samples_generator import make_blobs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gower Library\n",
    "This library calculates the distance between records taking into account that categorical values are either the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library to deal with categorical values without onehot encoding\n",
    "!pip install gower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you to Robert Uleman for providing the function in the following two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from gower import gower_matrix\n",
    "def get_linkage(df, weights, cat_columns):\n",
    "    '''\n",
    "    Perform hierarchical/agglomerative clustering.\n",
    "    Parameters:\n",
    "        df          input DataFrame\n",
    "        weights     list of weights, same length and in same order as df.columns\n",
    "        cat_columns list of booleans, same length and in same order as df_columns,\n",
    "                    indicating whether a column is categorical or not\n",
    "    Returns:\n",
    "        The hierarchical clustering encoded as a linkage matrix\n",
    "    '''\n",
    "    pairwise_dist = gower_matrix(data_x=df, weight=weights, cat_features=cat_columns)\n",
    "    return linkage(squareform(pairwise_dist), method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def make_weights(df, default=1, **kwargs):\n",
    "    '''\n",
    "    Create a {column_name:weight} ordered dictionary for Gower distance computation.\n",
    "    Gower method depends on list parameters coordinated in order, so must preserve column order.\n",
    "    Parameters:\n",
    "        df        Pandas Dataframe. Create a weight for each of the columns\n",
    "        default   Default weight: columns get this weight unless explicitly overridden\n",
    "        kwargs    column_name=weight pairs to override the default assignment\n",
    "    Returns:\n",
    "        An OrderedDict of {column_name:weight} pairs for all columns in df\n",
    "    '''\n",
    "    # Set default weight of 1 for all columns\n",
    "    weights = OrderedDict(zip(df.columns, [default]*len(df.columns)))\n",
    "    # Override the defaults with provided values (NOTE: this is an in-place method)\n",
    "    weights.update(kwargs)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_scaled)\n",
    "df.columns = encoded_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_categorical_columns = ['TRAFFIC_CONTROL_DEVICE_ID','DEVICE_CONDITION_ID', \n",
    "                       'WEATHER_CONDITION_ID', 'LIGHTING_CONDITION_ID','FIRST_CRASH_TYPE_ID',\n",
    "                       'TRAFFICWAY_TYPE_ID', 'ALIGNMENT_ID','ROADWAY_SURFACE_COND_ID', \n",
    "                       'ROAD_DEFECT_ID', 'REPORT_TYPE_ID','CRASH_TYPE_ID',\n",
    "                       \"INTERSECTION_RELATED_I\",\"NOT_RIGHT_OF_WAY_I\",\"HIT_AND_RUN_I\",\n",
    "                       'DAMAGE_ID', 'PRIM_CONTRIBUTORY_CAUSE_ID','SEC_CONTRIBUTORY_CAUSE_ID'\n",
    "                       \"STREET_DIRECTION\",\"PHOTOS_TAKEN_I\",\n",
    "                       \"STATEMENTS_TAKEN_I\",\"DOORING_I\",\"WORK_ZONE_I\",\n",
    "                       'WORK_ZONE_TYPE_ID',\"WORKERS_PRESENT_I\",'MOST_SEVERE_INJURY_ID'\n",
    "                      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.thinkdatascience.com/post/2019-12-16-introducing-python-package-gower/\n",
    "weights_dict = make_weights(df)\n",
    "Z = get_linkage(df,\n",
    "                weights    =np.asarray(list(weights_dict.values())),\n",
    "                cat_columns=df.columns.isin(w_categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "plt.figure(figsize=(15, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "dend = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff=0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "plt.axhline(y=cutoff, color='r', linestyle='--')\n",
    "dend = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = fcluster(Z, cutoff, 'distance')\n",
    "nb_clusters = vals.max()\n",
    "clusters_counts = np.unique(vals,return_counts=True)\n",
    "print(\"Number of clusters: {0}\".format(nb_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the group to the data and get only the ones that were grouped\n",
    "data_pd['hgroup'] = -1\n",
    "data_pd.loc[data_pd.drop([\"LATITUDE\",\"LONGITUDE\"], axis=1).dropna().index, ['hgroup']] = vals\n",
    "pdata_pd = data_pd.loc[data_pd.hgroup > -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the clusters on a map\n",
    "Display each address as a point with a specific color by cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the cluster colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an array of 4 dimensions arrays\n",
    "import matplotlib.cm as cm\n",
    "colors = cm.rainbow(np.linspace(0, 1, nb_clusters + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbcolors = []\n",
    "for v in colors :\n",
    "    col = np.floor(v * 255)\n",
    "    r = int(col[0])\n",
    "    g = int(col[1])\n",
    "    b = int(col[2])\n",
    "    rgbcolors.append('#' + '{0:#08x}'.format(((r * 65536) + (g * 256) + b))[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://medium.com/@bobhaffner/creating-a-legend-for-a-folium-map-c1e0ffc34373\n",
    "# Also: https://fontawesome.com/v4.7.0/icons/\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "     top: 50px; right: 50px; width: 150px; height: 150px; \n",
    "     border:2px solid grey; z-index:9999; font-size:14px;\n",
    "     \">&nbsp; <u><b>CLUSTERS</b></u> <br/>\n",
    "'''\n",
    "for v in range(nb_clusters) :\n",
    "    legend_html = legend_html + \\\n",
    "    '''\n",
    "    &nbsp; <i class=\"fa fa-square\" style=\"color:{2}\"></i>\n",
    "    &nbsp; ({0}) Cluster-{1} &nbsp;<br/>\n",
    "    '''.format(clusters_counts[1][v],(v + 1),rgbcolors[v])\n",
    "legend_html = legend_html + '</div>'\n",
    "# print(legend_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate a center point for the map\n",
    "latlong = pdata_pd[['LATITUDE','LONGITUDE']].mean()\n",
    "\n",
    "chi_map = folium.Map(location=[latlong[0], latlong[1]], zoom_start=10, width=\"100%\", height=\"100%\")\n",
    "\n",
    "for row in pdata_pd.itertuples() :\n",
    "    tooltip_content=\"BEAT_OF_OCCURRENCE: {0}<br/>Cluster: {1}<br/>\".format(\n",
    "        row.BEAT_OF_OCCURRENCE,row.hgroup)\n",
    "    folium.Circle(\n",
    "    radius=10,\n",
    "    location=[row.LATITUDE, row.LONGITUDE],\n",
    "    popup=row.hgroup,\n",
    "    color=rgbcolors[row.hgroup - 1],\n",
    "    tooltip=tooltip_content,\n",
    "    fill=True,\n",
    "    fill_color=rgbcolors[row.hgroup - 1]\n",
    ").add_to(chi_map)\n",
    "chi_map.get_root().html.add_child(folium.Element(legend_html))  \n",
    "chi_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "Density-based spatial clustering.<br/>\n",
    "Locates regions of high density that are separated from one another by regions of low density.\n",
    "\n",
    "info:\n",
    "- https://scikit-learn.org/stable/modules/clustering.html#clustering\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster\n",
    "- https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\n",
    "- https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the location information\n",
    "sql = \"\"\"\n",
    "  SELECT LATITUDE, LONGITUDE\n",
    "  FROM CHICAGO.ChicagoAccidents TABLESAMPLE SYSTEM(20)\n",
    "  ;\n",
    "\"\"\"\n",
    "\n",
    "data_pd = pd.read_sql(sql, pconn)\n",
    "print(\"Number of records: {0}\".format(data_pd.shape[0]))\n",
    "data_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = data_pd[['LATITUDE','LONGITUDE']].to_numpy()\n",
    "\n",
    "# [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]\n",
    "# Use default values except for eps\n",
    "db = DBSCAN(eps=0.001, min_samples=20, metric='euclidean', metric_params=None,\n",
    "            algorithm='auto', leaf_size=30, p=None, n_jobs=None).fit(data_np)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "# Plot result\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = data_np[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    #xy = data_np[class_member_mask & ~core_samples_mask]\n",
    "    #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "    #         markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "colors = cm.rainbow(np.linspace(0, 1, n_clusters_ + 1))\n",
    "\n",
    "rgbcolors = []\n",
    "for v in colors :\n",
    "    col = np.floor(v * 255)\n",
    "    r = int(col[0])\n",
    "    g = int(col[1])\n",
    "    b = int(col[2])\n",
    "    rgbcolors.append('#' + '{0:#08x}'.format(((r * 65536) + (g * 256) + b))[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the average center of each group\n",
    "\n",
    "all_recs = np.append(data_np, db.labels_.reshape((db.labels_.shape[0],1)), 1)\n",
    "all_recs = all_recs[np.logical_or.reduce([db.labels_ > -1])]\n",
    "unique_elements, counts_elements = np.unique(all_recs[...,2], return_counts=True)\n",
    "\n",
    "results=[]\n",
    "for x in sorted(np.unique(all_recs[...,2])):\n",
    "    results.append([np.average(all_recs[np.where(all_recs[...,2]==x)][...,0]), \n",
    "                    np.average(all_recs[np.where(all_recs[...,2]==x)][...,1]), x])\n",
    "\n",
    "latlong = all_recs.mean(axis=0)[0:2]\n",
    "\n",
    "chi_map = folium.Map(location=[latlong[0], latlong[1]], zoom_start=10, width=\"90%\", height=\"90%\")\n",
    "\n",
    "for coord in results:\n",
    "    tooltip_content=\"Cluster: {0}, count: {1}\".format(coord[2].astype(int),counts_elements[coord[2].astype(int)] )\n",
    "    folium.Circle(radius=10,\n",
    "                  location=[coord[0], coord[1]],\n",
    "                  # popup=row.hgroup,\n",
    "                  color=rgbcolors[coord[2].astype(int) - 1],\n",
    "                  tooltip=tooltip_content,\n",
    "                  fill=True,\n",
    "                  fill_color=rgbcolors[coord[2].astype(int) - 1]\n",
    "    ).add_to(chi_map)\n",
    "    \n",
    "chi_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the point in each cluster to a polygon/multipolygon\n",
    "see: https://shapely.readthedocs.io/en/stable/manual.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas 2>&1 >pipgeopandas.txt\n",
    "import geopandas as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd['cgroup'] = db.labels_\n",
    "data_pd['cnt'] = [1] * db.labels_.shape[0]\n",
    "\n",
    "geo_gpd = gp.GeoDataFrame(data_pd, geometry=gp.points_from_xy(data_pd.LONGITUDE, data_pd.LATITUDE))\n",
    "geo_gpd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the DBSCAN eps value for the buffer\n",
    "group_gpd = geo_gpd[['cgroup','geometry','cnt']].dissolve(by='cgroup', aggfunc='sum').reset_index().drop([0])\n",
    "# group2_gpd = gp.GeoDataFrame(group_gpd[['cgroup','cnt']],geometry=group_gpd.geometry.buffer(0.003))\n",
    "group2_gpd = gp.GeoDataFrame(group_gpd[['cgroup','cnt']],geometry=group_gpd.geometry.convex_hull)\n",
    "group2_gpd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latlong = geo_gpd[['LATITUDE','LONGITUDE']].mean(axis=0)\n",
    "\n",
    "chi_map = folium.Map(location=[latlong[0], latlong[1]], zoom_start=10, width=\"90%\", height=\"90%\")\n",
    "\n",
    "# geom2 = chi_gdf[chi_gdf['NAME']=='Chicago'].reset_index()\n",
    "\n",
    "for ix in range(group2_gpd['cnt'].count()) :\n",
    "    folium.GeoJson(\n",
    "        group2_gpd.iloc[ix]['geometry'],\n",
    "        name=\"cluster-{0}\".format(group2_gpd.iloc[ix]['cgroup']),\n",
    "        tooltip=\"Cluster: {0}, count: {1}\".format(group2_gpd.iloc[ix]['cgroup'],group2_gpd.iloc[ix]['cnt'] )\n",
    "    ).add_to(chi_map)\n",
    "\n",
    "\n",
    "folium.LayerControl().add_to(chi_map)\n",
    "chi_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
