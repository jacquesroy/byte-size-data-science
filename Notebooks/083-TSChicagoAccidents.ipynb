{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <tr><td>\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a></td><td>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.</td>\n",
    "    </tr>\n",
    "    <tr><td>Jacques Roy, Byte Size Data Science</td><td> </td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Exploration\n",
    "In this notebook, we look at timeseries to get a feel for what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube video related to this notebook\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/fa7rHy7YmWU?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed in the notebook\n",
    "# import urllib3, requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import dateutil.parser\n",
    "\n",
    "# import io\n",
    "\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.patches lets us create colored patches, which we can use for legends in plots\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "In this section, we get data from the city of Chicago.\n",
    "\n",
    "Once we have the data, we can save it in a file in the project so we don't have to download it again.\n",
    "This is recommended since this data will be used in future notebooks.\n",
    "\n",
    "We'll get the data dynamically using the socrata API. See also: <a href=\"https://github.com/jacquesroy/byte-size-data-science/blob/master/Notebooks/W005-FindingData.ipynb\" target=\"_blank\">W005-FindingData.ipyng</a>\n",
    "\n",
    "The dataset attributes include:\n",
    "\n",
    "|  |  |  |  |\n",
    "| :--- | :--- | :--- | :--- \n",
    "| `alignment` | `beat_of_occurrence` | *crash_date* | `crash_date_est_i`\n",
    "| *crash_day_of_week* | *crash_hour* | *crash_month* | `crash_record_id`\n",
    "|`crash_type` | `damage` | `date_police_notified` | `device_condition`\n",
    "| `dooring_i` | `first_crash_type` | `hit_and_run_i` | `injuries_fatal`\n",
    "| `injuries_incapacitating` | `injuries_no_indication` | | \n",
    "| `injuries_non_incapacitating` | `injuries_reported_not_evident` | | |\n",
    "| `injuries_total` | `injuries_unknown` | `intersection_related_i` | `lane_cnt`\n",
    "| **`latitude`** | `lighting_condition` | `location` | **`longitude`**\n",
    "| `most_severe_injury` | `num_units` | `photos_taken_i` | \n",
    "| `posted_speed_limit` | `prim_contributory_cause` | `private_property_i` | \n",
    "| `rd_no` | `report_type` | `road_defect` | `roadway_surface_cond`\n",
    "| `sec_contributory_cause` | `statements_taken_i` | `street_direction` | \n",
    "| **`street_name`** | **`street_no`** | `traffic_control_device` | `trafficway_type`\n",
    "| `weather_condition` | `work_zone_i` | `work_zone_type` | `workers_present_i` \n",
    "\n",
    "For our purpose, we limit ourselves to a small subset of these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library used to read datasets\n",
    "# https://github.com/xmunoz/sodapy\n",
    "!pip install sodapy 2>&1 >pipsodapy.txt\n",
    "\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofchicago.org\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit the data we read\n",
    "We'll limit ourselves to 2019 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# If we wanted to do today:\n",
    "# We are using a fix date for future comparisons\n",
    "two_years = (date(2019,1,1)).strftime('%Y-%m-%d')\n",
    "where = \"crash_date >= '{}'and crash_date < '{}'\".format(two_years, date(2021,1,1))\n",
    "select = \"crash_date,crash_month,crash_day_of_week,crash_hour,injuries_fatal,injuries_total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if\n",
    "select = \"crash_date,crash_month,crash_day_of_week,crash_hour,injuries_fatal,injuries_total\"\n",
    "\n",
    "crashes_df = pd.DataFrame(client.get(\"85ca-t3if\", select=select,where=where, limit=10000))\n",
    "offset = 10000\n",
    "result = client.get(\"85ca-t3if\", select=select,where=where, offset=offset, limit=10000)\n",
    "while (len(result) > 0) :\n",
    "    crashes_df = crashes_df.append(pd.DataFrame(result), sort=True)\n",
    "    offset += 10000\n",
    "    result = client.get(\"85ca-t3if\", select=select,where=where, offset=offset, limit=10000)\n",
    "\n",
    "print(\"Number of records: {}, number of columns: {}\".format(crashes_df.shape[0], crashes_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to proper data types\n",
    "The data returned through this interface is only character strings. We need to convert them to the proper types.\n",
    "\n",
    "The `crash_date` includes hour, minute, and second. We only need the date part.\n",
    "\n",
    "Note that `crash_day_of week` starts with 1 for Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# injuries_fatal and injuries_total are converted to floats becasue of issues with NaN/Null values\n",
    "crashes_df2 = crashes_df.astype({'crash_date': 'datetime64[ns]' , 'crash_month': int, 'crash_day_of_week': int,\n",
    "                                 'crash_hour': int,'injuries_fatal': float, 'injuries_total': float})\n",
    "crashes_df2['crash_date'] = crashes_df2['crash_date'].dt.floor('d') # Keep only the date part\n",
    "\n",
    "crashes_df2['injuries_fatal'] = crashes_df2['injuries_fatal'].fillna(0)\n",
    "crashes_df2['injuries_total'] = crashes_df2['injuries_total'].fillna(0)\n",
    "crashes_df2['injuries_fatal'] = crashes_df2['injuries_fatal'].astype(int)\n",
    "crashes_df2['injuries_total'] = crashes_df2['injuries_total'].astype(int)\n",
    "\n",
    "crashes_df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data to Cloud Storage\n",
    "You need to import your cloud storage credentials.\n",
    "\n",
    "The easiest way is to open the file section on the right and use `Insert to code`\n",
    "to add the credentials to an empty cell.<br/>\n",
    "Make sure the name of the variable is `credentials`\n",
    "and not `credentials_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials = {\n",
    "    'IAM_SERVICE_ID': 'iam-ServiceId-9d918923-e68f-46df-b2ab-3c23479e0bee',\n",
    "    'IBM_API_KEY_ID': 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',\n",
    "    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'BUCKET': 'bscs2project-donotdelete-pr-vafl0cosn5bcq1',\n",
    "    'FILE': 'titanic.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config\n",
    "\n",
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['ENDPOINT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save crashes\n",
    "crashes_df2.to_csv('crashes.csv',index=False)\n",
    "# Save the file to COS\n",
    "cos.upload_file('crashes.csv', Bucket=credentials['BUCKET'],Key='crashes.csv')\n",
    "!rm crashes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the file to the project\n",
    "At this point, the file is added to your cloud storage. You need to also add it to your project.\n",
    "\n",
    "When you are in the project, open the files tab, select `crashed.csv` and add it to the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data from the Cloud Object Storage\n",
    "Instead of through Socrata if the file was created.\n",
    "\n",
    "Select an empty cell then open the file tab, use `insert to code` and select `pandas DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_f4a37e7b459c4faa8b0ceeeb172a28a7 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='CvdDbJdbPIpTlcNRUrp6',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_f4a37e7b459c4faa8b0ceeeb172a28a7.get_object(Bucket='bscs2project-donotdelete-pr-vafl0cosn5bcq1',Key='crashes.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "crashes_df2 = pd.read_csv(body)\n",
    "crashes_df2['crash_date'] = pd.to_datetime(crashes_df2['crash_date'])\n",
    "print('Number of records: {}'.format(crashes_df2.shape[0]))\n",
    "crashes_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping the data - Daily\n",
    "We need to group the data per day (for now) and differentiate between fatalities, injuries, and only property damage.\n",
    "\n",
    "For future analysis, we could also add the number of injuries and number of fatalities per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to reflect the type of accident\n",
    "crashes_df2['fatalities'] = (crashes_df2.injuries_fatal > 0).astype(int)\n",
    "crashes_df2['injuries'] = ((crashes_df2.injuries_total > 0 ) & (crashes_df2.injuries_fatal == 0)).astype(int)\n",
    "crashes_df2['damage'] = ((crashes_df2.injuries_total == 0) & (crashes_df2.injuries_fatal == 0)).astype(int)\n",
    "crashes_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate accidents \n",
    "crashes_df3 = crashes_df2[['crash_date','crash_day_of_week','crash_month','fatalities','injuries','damage']].\\\n",
    "                        groupby(by='crash_date').agg({'crash_day_of_week': 'min','crash_month': 'min',\\\n",
    "                                                      'fatalities': 'sum','injuries': 'sum','damage': 'sum' } )\n",
    "# Add a total column to have the total number of accidents per day\n",
    "crashes_df3['total'] = crashes_df3[['fatalities','injuries','damage']].sum(axis=1)\n",
    "crashes_df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the daily accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset = int((9 - crashes_df3.iloc[0].crash_day_of_week) % 7) # Offset to start on a Monday\n",
    "offset = 0\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "crashes_df3['total'][offset:].plot(kind='line',style='.-',grid=True)\n",
    "plt.title('Chicago Daily accident totals from {} to {}'.format(crashes_df3.index[offset].date(),crashes_df3.index[crashes_df3.shape[0] - 1].date()) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First observation\n",
    "From plotting the timeseries, we can see that something significant happened in March 2020.\n",
    "\n",
    "This coincide with the beginning of pandemic lockdown.\n",
    "\n",
    "We also see that from April to July 2020, the daily accidents went back up almost to the pre-pandemic level. \n",
    "We can see three separate sections to the time series:\n",
    "- beginning to March 2020\n",
    "- March 2020 to July 2020\n",
    "- July 2020 to the end\n",
    "\n",
    "Still, from July 2020, we can see that we are still in an uncertain period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on 2019\n",
    "For now, we'll look at the data from 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2019 = crashes_df3[(crashes_df3.index >= '2019-01-01') & (crashes_df3.index < '2020-01-01')].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add linear regression for the total number of accidents \n",
    "We can then see if there is a upward or downward trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regressor = LinearRegression()  # create object for the class\n",
    "X = pd.Series(range(df2019.shape[0])).values.reshape(df2019.shape[0],1)\n",
    "Y = df2019['total'].values.reshape(df2019.shape[0],1)\n",
    "\n",
    "linear_regressor.fit(X, Y)  # perform linear regression\n",
    "Y_pred = linear_regressor.predict(X)  # make predictions\n",
    "df2019['regression'] = np.reshape(Y_pred, Y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset = int((9 - df2019.iloc[0].crash_day_of_week) % 7) # If we wanted to start on a Monday\n",
    "offset = 0\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "df2019['total'][offset:].plot(kind='line',style='.-',grid=True)\n",
    "df2019['regression'][offset:].plot(kind='line', color='red',grid=True) # linewidth=3\n",
    "\n",
    "plt.title('Daily accident totals from {} to {}'.format(df2019.index[offset].date(),df2019.index[df2019.shape[0] - 1].date()) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if the regression is flat\n",
    "print(\n",
    "    \"Regression at the beginning: {:7.3f} , end: {:7.3f}, difference: {:4.3f}\".format(\n",
    "        df2019.regression[0],\n",
    "        df2019.regression[df2019.shape[0] - 1],\n",
    "        df2019.regression[df2019.shape[0] - 1] - df2019.regression[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution\n",
    "We need to adjust the totals based on the trend (regression slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "(df2019.total + (df2019.regression - df2019.regression[0])).hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "- This looks like a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize outliers\n",
    "For example, we can see that January 12 and November 12 are obviously outliers. Let's be more formal.\n",
    "\n",
    "#### <font color='red'>Warning!</font>\n",
    "\n",
    "We can do that because the data appears to be stable throughout the year.<br/>\n",
    "This is not possible in all cases. For example, if we had both years (2019,2020), that would not work.\n",
    "\n",
    "We'll see more on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_dev = df2019.total.std().item()\n",
    "median = df2019.total.median().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset = int((9 - df2019.iloc[0].crash_day_of_week) % 7)\n",
    "offset = 0\n",
    "df2019_cnt = df2019.shape[0] - offset\n",
    "\n",
    "median_ser = pd.Series([median] * df2019_cnt)\n",
    "median_ser.index = df2019.index[offset:]\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "df2019['total'][offset:].plot(kind='line',style='.-',grid=True)\n",
    "df2019['regression'][offset:].plot(kind='line', color='red',grid=True,legend=False,label=\"regr\") # linewidth=3\n",
    "median_ser.plot(kind='line', color='cyan',grid=True,legend=False,label=\"median\") # linewidth=3\n",
    "\n",
    "(median_ser + standard_dev).plot(kind='line', color='olive',grid=True,legend=False,label=\"+1-std\") # linewidth=3\n",
    "(median_ser - standard_dev).plot(kind='line', color='olive',grid=True,legend=False, label=\"\") # linewidth=3\n",
    "\n",
    "(median_ser + (2 * standard_dev)).plot(kind='line', color='green',grid=True,legend=False,label=\"2-std\") # linewidth=3\n",
    "(median_ser - (2 * standard_dev)).plot(kind='line', color='green',grid=True,legend=False, label=\"\") # linewidth=3\n",
    "\n",
    "(median_ser + (3 * standard_dev)).plot(kind='line', color='orange',grid=True,legend=False,label=\"3-std\") # linewidth=3\n",
    "(median_ser - (3 * standard_dev)).plot(kind='line', color='orange',grid=True,legend=False, label=\"\") # linewidth=3\n",
    "\n",
    "plt.title('Daily accident totals from {} to {}'.format(df2019.index[0].date(),df2019.index[df2019.shape[0] - 1].date()) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "In a standard distribution (which we seem to have), we see:\n",
    "- Within one standard deviation   : 68.2% of values\n",
    "- Within two standard deviations  : 95.4% of values\n",
    "- Within three standard deviations: 99.6% of values\n",
    "\n",
    "We have a few point passed three standard deviations and several more passed two.<br/>\n",
    "Of course, some of them are in the fewer accidents category so we're happy with that.<br/>\n",
    "We may want to find out why these values are outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are some days of the week worse than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days=['Sunday', 'Monday','Tuesday','Wednesday','Thursday', 'Friday','Saturday']\n",
    "avg_df = df2019[['crash_day_of_week','total']].groupby('crash_day_of_week').mean()\n",
    "avg_df.index = days\n",
    "print(avg_df)\n",
    "avg_df.plot(kind='bar', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the worst always Friday?\n",
    "TODO: Need to set the dates to the same week date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the dates to the same day of the week (Tuesday here)\n",
    "df2019_2 =df2019.copy(deep=True)\n",
    "df2019_2.index = df2019.index - pd.to_timedelta((df2019['crash_day_of_week'] - 3), unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "df2019_2[df2019_2.crash_day_of_week == 1]['total'].plot(kind='line',grid=True,legend=True,label=\"Sunday\")\n",
    "df2019_2[df2019_2.crash_day_of_week == 2]['total'].plot(kind='line',grid=True,legend=True,label=\"Monday\")\n",
    "df2019_2[df2019_2.crash_day_of_week == 3]['total'].plot(kind='line',grid=True,legend=True,label=\"Tuesday\")\n",
    "df2019_2[df2019_2.crash_day_of_week == 4]['total'].plot(kind='line',grid=True,legend=True,label=\"Wednesday\")\n",
    "df2019_2[df2019_2.crash_day_of_week == 5]['total'].plot(kind='line',grid=True,legend=True,label=\"Thursday\")\n",
    "df2019_2[df2019_2.crash_day_of_week == 6]['total'].plot(kind='line',grid=True,legend=True,label=\"Friday\")\n",
    "df2019_2[df2019_2.crash_day_of_week == 7]['total'].plot(kind='line',grid=True,legend=True,label=\"Saturday\")\n",
    "\n",
    "plt.title('Daily accident totals from {} to {} by day of the week'.format(df2019.index[0].date(),df2019.index[df2019.shape[0] - 1].date()) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary time series?\n",
    "Can we consider 2019 a stationary time series?\n",
    "\n",
    "A lot of time series analysis assume a stationary one.\n",
    "\n",
    "To check this, we use the augmented Dickey-Fuller test.\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\n",
    "\n",
    "We are using a confidence level of 95% or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.graphics.tsaplots as sgt\n",
    "import statsmodels.tsa.stattools as sts\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for color display\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "def is_stationary(adf, name) :\n",
    "    if (adf[1] < 0.5) :\n",
    "        if (adf[0] < adf[4]['1%']) :\n",
    "            print('The {} time series is stationary within the 1% margin'.format(name))\n",
    "        elif (adf[0] < adf[4]['5%']) :\n",
    "            print('The {} time series is stationary within the 5% margin'.format(name))\n",
    "        else :\n",
    "            printmd(\"The {} time series is <span style='color:{}'>**NOT**</span> stationary\".format(name,'red'))\n",
    "    else :\n",
    "        printmd(\"The {} time series is <span style='color:{}'>**NOT**</span> stationary\".format(name,'red'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = sts.adfuller(df2019.total)\n",
    "print('adf: {}\\npvalue: {}\\nusedlag: {}\\nnubs: {}'.format(adf[0],adf[1],adf[2],adf[3]))\n",
    "print('critical values: {}\\nicbest: {}'.format(adf[4],adf[5]))\n",
    "is_stationary(adf, 'total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info:\n",
    "The `pvalue` is below 0.5 and `adf` is smaller than the 5% value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
