{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<IMG SRC=\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\" ALT=\"BSDS Banner\" WIDTH=1195 HEIGHT=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <tr><td>\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a></td><td>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.</td>\n",
    "    </tr>\n",
    "    <tr><td>Jacques Roy, Byte Size Data Science</td><td> </td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Simple use case trying to answer:\n",
    "- Where are the accidents in the Chicago area?\n",
    "- Where should we prioritize our efforts?\n",
    "\n",
    "For this, we'll use multiple clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube video related to this notebook\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.youtube.com/embed/MhtuJfYNYdo?rel=0&amp;controls=0&amp;showinfo=0\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library used to read datasets\n",
    "# https://github.com/xmunoz/sodapy\n",
    "!pip install sodapy 2>&1 >pipsodapy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install geopandas for geo objects support\n",
    "!pip install geopandas 2>&1 >pipgeopandas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install folium for map rendering\n",
    "!pip install folium 2>&1 >foliumpip.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed in the notebook\n",
    "# import urllib3, requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import io\n",
    "\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.patches lets us create colored patches, which we can use for legends in plots\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "from sodapy import Socrata\n",
    "import geopandas as gp\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "We'll get the data dynamically using the socrata API. See also: <a href=\"https://github.com/jacquesroy/byte-size-data-science/blob/master/Notebooks/W005-FindingData.ipynb\" target=\"_blank\">W005-FindingData.ipyng</a>\n",
    "\n",
    "To limit our dataset, we are reading the last 6 months of data.\n",
    "\n",
    "The dataset attributes include:\n",
    "```\n",
    "  'alignment',                   'beat_of_occurrence',      'crash_date',           'crash_date_est_i',\n",
    "  'crash_day_of_week',           'crash_hour',              'crash_month',          'crash_record_id',\n",
    "  'crash_type',                  'damage',                  'date_police_notified', 'device_condition',\n",
    "  'dooring_i',                   'first_crash_type',        'hit_and_run_i',        'injuries_fatal',\n",
    "  'injuries_incapacitating',     'injuries_no_indication',\n",
    "  'injuries_non_incapacitating', 'injuries_reported_not_evident',\n",
    "  'injuries_total',              'injuries_unknown',        'intersection_related_i',\n",
    "  'lane_cnt',                    'latitude',                'lighting_condition',   'location', 'longitude',\n",
    "  'most_severe_injury',          'num_units',               'photos_taken_i',\n",
    "  'posted_speed_limit',          'prim_contributory_cause', 'private_property_i',\n",
    "  'rd_no',                       'report_type',             'road_defect',          'roadway_surface_cond',\n",
    "  'sec_contributory_cause',      'statements_taken_i',      'street_direction',\n",
    "  'street_name',                 'street_no',               'traffic_control_device', 'trafficway_type',\n",
    "  'weather_condition',           'work_zone_i',             'work_zone_type',\n",
    "  'workers_present_i'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofchicago.org\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# If we wanted to do today:\n",
    "# six_months = (date.today() - relativedelta(months=+6)).strftime('%Y-%m-%d')\n",
    "# We are using a fix date for future comparisons\n",
    "six_months = (date(2021,1,15) - relativedelta(months=+6)).strftime('%Y-%m-%d')\n",
    "where = \"crash_date > '{}'\".format(six_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if\n",
    "crashes_df = pd.DataFrame(client.get(\"85ca-t3if\", where=where, limit=10000))\n",
    "offset = 10000\n",
    "result = client.get(\"85ca-t3if\", where=where, offset=offset, limit=10000)\n",
    "while (len(result) > 0) :\n",
    "    crashes_df = crashes_df.append(pd.DataFrame(result), sort=True)\n",
    "    offset += 10000\n",
    "    result = client.get(\"85ca-t3if\", where=where, offset=offset, limit=10000)\n",
    "\n",
    "print(\"Number of records: {}, number of columns: {}\".format(crashes_df.shape[0], crashes_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting columns\n",
    "In this example, we keep it simple and only work with the longitude and latitude.\n",
    "\n",
    "When using clustering, we are not limited to positions such as longitude and latiutude.\n",
    "We can iuse other attributes. For example, we can look at injuries and fatal injuries for example.\n",
    "\n",
    "For the clustering to make sense, we need to make sure that all the values are normalized.\n",
    "This way one value does not overwhelm the clustering. More on modeling later.\n",
    "\n",
    "Here's what we need to do once we selected our columns:\n",
    "- Reading the data from Socrata only gives us character strings, we need to convert to the proper types\n",
    "- We also need to remove the records that don't have longitude and latitude.<br/>\n",
    "This demonstrates that we always need to do data cleansing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has to be a better way to do this...\n",
    "crashes_df = crashes_df[['injuries_fatal','injuries_total','latitude','longitude']]\n",
    "# convert 'injuries_fatal' and 'injuries_total' to float otherwide, int causes problems.\n",
    "crashes_df = crashes_df.astype({'injuries_fatal': float, 'injuries_total': float,\n",
    "                                'latitude': float, 'longitude': float})\n",
    "crashes_df = crashes_df.dropna()\n",
    "crashes_df = crashes_df[crashes_df['longitude'] != 0]\n",
    "crashes_df = crashes_df[crashes_df['latitude'] != 0]\n",
    "\n",
    "#divide dataset into accident categories: fatal, non-fatal but with injuries, none of the above\n",
    "killed_df = crashes_df[crashes_df['injuries_fatal']>0]\n",
    "injured_df = crashes_df[np.logical_and(crashes_df['injuries_total']>0, crashes_df['injuries_fatal']==0)]\n",
    "killed_or_injured_df = killed_df.append(injured_df)\n",
    "nothing_df = crashes_df[np.logical_and(crashes_df['injuries_fatal']==0, crashes_df['injuries_total']==0)]\n",
    "\n",
    "print(\"Number of records: {}\".format(crashes_df.shape[0]))\n",
    "print(\"Number of fatal accidents: {}\".format(killed_df.shape[0]))\n",
    "print(\"Number of injury accidents: {}\".format(injured_df.shape[0]))\n",
    "print(\"Number of no-injury accidents: {}\".format(nothing_df.shape[0]))\n",
    "\n",
    "crashes_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot\n",
    "Create a visualization of the accidents. Note that this is not a map.\n",
    "\n",
    "Having a graphical representation of our data can give us some insights on how to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use calculated values for the plot limits (border)\n",
    "minlong = crashes_df['longitude'].min(axis=0) - 0.005\n",
    "maxlong = crashes_df['longitude'].max(axis=0) + 0.005\n",
    "minlat = crashes_df['latitude'].min(axis=0) - 0.005\n",
    "maxlat = crashes_df['latitude'].max(axis=0) + 0.005\n",
    "print(\"min, max, longitude, latitude: {}, {}, {}, {}\".format(minlong,maxlong,minlat,maxlat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I've also done subplots in notebook 61\n",
    "nb_rows = 1\n",
    "nb_plots = 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nb_rows, ncols=2)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "axes[0].scatter(crashes_df.longitude, crashes_df.latitude, color='darkseagreen', alpha=0.05, s=2)\n",
    "axes[0].title.set_text('Motor Vehicle Accidents in Chicago (last six months)')\n",
    "axes[0].set_xlabel('Longitude', labelpad = 5)\n",
    "axes[0].set_ylabel('latitude', labelpad = 5)\n",
    "\n",
    "\n",
    "axes[1].scatter(nothing_df.longitude, nothing_df.latitude, color='blue', alpha=0.04, s=2)\n",
    "axes[1].scatter(injured_df.longitude, injured_df.latitude, color='yellow', alpha=0.12, s=2)\n",
    "axes[1].scatter(killed_df.longitude, killed_df.latitude, color='red', alpha=1, s=2)\n",
    "\n",
    "#create legend\n",
    "blue_patch = mpatches.Patch( label='car body damage', color='blue')\n",
    "yellow_patch = mpatches.Patch(color='yellow', label='personal injury')\n",
    "red_patch = mpatches.Patch(color='red', label='lethal accidents')\n",
    "axes[1].legend([blue_patch, yellow_patch, red_patch],('car body damage', 'personal injury', 'fatal accidents'), \n",
    "           loc='upper left', prop={'size':10})\n",
    "axes[1].title.set_text('Severity of Motor Vehicle Collisions in Chicago')\n",
    "axes[1].set_xlabel('Longitude', labelpad = 5)\n",
    "axes[1].set_ylabel('latitude', labelpad = 5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we see?\n",
    "- The Chicago area has a lot of accidents and they are all over the place.\n",
    "- The first graph shows darket color in areas. This implies more accident density in some place.\n",
    "- An overwhelming number of accident do not involve injurues or death.\n",
    "- Reducing the opacity (alpha) of each point improved the information proivided by the map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the graph for only injuries and deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've done subplots in notebook 61\n",
    "nb_rows = 1\n",
    "nb_plots = 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nb_rows, ncols=2)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "axes[0].scatter(crashes_df.longitude, crashes_df.latitude, alpha=0.05, s=4, color='darkseagreen')\n",
    "axes[0].title.set_text('Motor Vehicle Accidents in Chicago (last six months)')\n",
    "axes[0].set_xlabel('Longitude', labelpad = 5)\n",
    "axes[0].set_ylabel('latitude', labelpad = 5)\n",
    "\n",
    "\n",
    "axes[1].scatter(injured_df.longitude, injured_df.latitude, alpha=0.1, s=1, color='blue')\n",
    "axes[1].scatter(killed_df.longitude, killed_df.latitude, color='red', s=5)\n",
    "\n",
    "#create legend\n",
    "blue_patch = mpatches.Patch( label='personal injuries', color='blue')\n",
    "red_patch = mpatches.Patch(color='red', label='lethal accidents')\n",
    "axes[1].legend([blue_patch, red_patch],('personal injuries', 'fatal accidents'), \n",
    "           loc='upper left', prop={'size':10})\n",
    "axes[1].title.set_text('Severity of Motor Vehicle Collisions in Chicago')\n",
    "axes[1].set_xlabel('Longitude', labelpad = 5)\n",
    "axes[1].set_ylabel('latitude', labelpad = 5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with K-Means\n",
    "Apply it only to accidents with fatalities for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distortions = []\n",
    "K = range(1,60,1)\n",
    "df = killed_df[['longitude','latitude']]\n",
    "for k in K :\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(df)\n",
    "    distortions.append(sum(np.min(cdist(df, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method for optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 20 as the optimal K\n",
    "# K Means Cluster\n",
    "k=20\n",
    "model = KMeans(n_clusters=k)\n",
    "kmeans = model.fit(df)\n",
    "vals=[0] * k\n",
    "for i in kmeans.labels_ :\n",
    "    vals[i] = vals[i] + 1\n",
    "# Distribution between clusters\n",
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the clusters on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = df.copy(deep=True)\n",
    "data_pd['cgroup'] = kmeans.labels_\n",
    "data_pd['cnt'] = [1] * kmeans.labels_.shape[0]\n",
    "\n",
    "geo_gpd = gp.GeoDataFrame(data_pd, geometry=gp.points_from_xy(data_pd.longitude, data_pd.latitude))\n",
    "# geo_gpd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shapes for each cluster\n",
    "group_gpd = geo_gpd[['cgroup','geometry','cnt']].dissolve(by='cgroup', aggfunc='sum').reset_index()\n",
    "group2_gpd = gp.GeoDataFrame(group_gpd[['cgroup','cnt']],geometry=group_gpd.geometry.convex_hull)\n",
    "# group2_gpd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latlong = geo_gpd[['latitude','longitude']].mean(axis=0)\n",
    "\n",
    "chi_map = folium.Map(location=[latlong[0], latlong[1]], zoom_start=10, width=\"90%\", height=\"90%\")\n",
    "\n",
    "for ix in range(group2_gpd['cnt'].count()) :\n",
    "    folium.GeoJson(\n",
    "        group2_gpd.iloc[ix]['geometry'],\n",
    "        name=\"cluster-{0}\".format(group2_gpd.iloc[ix]['cgroup']),\n",
    "        tooltip=\"Cluster: {0}, count: {1}\".format(group2_gpd.iloc[ix]['cgroup'],group2_gpd.iloc[ix]['cnt'] )\n",
    "    ).add_to(chi_map)\n",
    "\n",
    "\n",
    "folium.LayerControl().add_to(chi_map)\n",
    "chi_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we learn?\n",
    "The k-means clustering for fatal accidents are not that useful.<br/>\n",
    "The number of accidents is too low to provide proper clustering. The clusters are too large for the number of accidents per cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use K-Means with weights\n",
    "Lets use both the fatal and injuries accidents.\n",
    "\n",
    "We will weigh the fatalities higher than the injuries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = killed_df[['longitude','latitude']].copy(deep=True)\n",
    "df1['weight'] = [10] * df1.shape[0]\n",
    "df2 = injured_df[['longitude','latitude']].copy(deep=True)\n",
    "df2['weight'] = [1] * df2.shape[0]\n",
    "\n",
    "df = df1.append(df2, ignore_index = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(20,500,10)\n",
    "for k in K :\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(df[['longitude','latitude']],sample_weight=df['weight'])\n",
    "    distortions.append(sum(np.min(cdist(df[['longitude','latitude']], kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method for optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More K-Means?\n",
    "At think at this point, we are done with K-means. We need to look at other ways to cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical\n",
    "Let's try building the clusters from the bottom up on fatalities and injuries.\n",
    "\n",
    "Even if we build the clusters from the bottom up, we can ask for a specific number of clusters (default 2).<br/>\n",
    "If the number of clusters in None, we can get the entire hierarchy.\n",
    "\n",
    "Additional parameters are avaialble to impact how clusters are merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import objects assuming the k-means section was skipped\n",
    "from scipy import ndimage \n",
    "from scipy.cluster import hierarchy \n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.spatial import distance_matrix \n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import manifold \n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=None,distance_threshold=0)\n",
    "clusters = ac.fit(df[['longitude','latitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linkage_matrix(model, **kwargs):\n",
    "    # Create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "    return(linkage_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linkage_matrix = get_linkage_matrix(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))  \n",
    "ret = dendrogram(linkage_matrix, truncate_mode='lastp', p=50, no_plot=False, orientation='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram\n",
    "In this figure (Dendogram) we see up to 50 clusters with the number of accidents in each.\n",
    "\n",
    "We could pick different numbers of clusters and display on a map to see if we get additional insights.\n",
    "Instead, we move on another typoe of clustering that may be more promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "Density-based spatial clustering.\n",
    "Locates regions of high density that are separated from one another by regions of low density.\n",
    "\n",
    "info:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/clustering.html#clustering\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster\n",
    "- https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\n",
    "- https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html\n",
    "\n",
    "The key statement in the model creation below is:\n",
    "\n",
    "```\n",
    "db = DBSCAN(eps=0.00015, min_samples=30).fit(crashes_df[['latitude','longitude']])\n",
    "\n",
    "```\n",
    "The parameters we really have to pay attention to are:\n",
    "- `eps` (default 0.5): The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "- `min_samples` (default 5): The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "- `metric` (default 'euclidean'): How to calculate the distance between points.\n",
    "\n",
    "The parameter that needs additional explanation is `eps`. How did we get to 0.001 ?\n",
    "\n",
    "The key is to consider that we are dealing with locations, and longitudes and latitudes.\n",
    "The distance has to be in the proper scale. In this example, we are looking for accident hotspots.\n",
    "They should be at an intersection or between two intersections.\n",
    "Luckily for us, we don't need to be too accurate.\n",
    "\n",
    "\n",
    "The assumption is that is is likely at an intersection. How large should we consider this intersection?\n",
    "\n",
    "A street is roughly 30 feet wide. If the accident is in the middle of the intersection, we should \n",
    "be ok using a distance of around 40 feet.\n",
    "\n",
    "In the Chicago area, the value 0.00015 represents roughly:\n",
    "- Horizontal (longitudinal) distance: 40 feet\n",
    "- Vertical (latitudinal) distance: 54 feet\n",
    "- Diagonal distance: 68 feet\n",
    "\n",
    "The earth is not quite round. Take a look at <a href=\"https://youtu.be/LKANJBxxtuQ\" target=\"_blank\">video 45</a> for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we aim for a number of clusters that is lower than the number of accidents with fatalities (<92)?\n",
    "# data_np = crashes_df[['latitude','longitude']].to_numpy()\n",
    "\n",
    "# eps=0.001 represents roughly \n",
    "db = DBSCAN(eps=0.00015, min_samples=18).fit(crashes_df[['latitude','longitude']])\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the cluster centers on a map\n",
    "Note that we display only the top 20 by selecting them from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = crashes_df[['latitude','longitude']].copy(deep=True)\n",
    "results_df['cluster'] = db.labels_\n",
    "results_df = results_df[results_df['cluster'] > -1].reset_index(drop=True) # remove noise points\n",
    "results_df = results_df.groupby('cluster').agg(latitude=pd.NamedAgg(column='latitude',aggfunc=\"mean\"), \n",
    "                                               longitude = pd.NamedAgg(column='longitude',aggfunc=\"mean\"), \n",
    "                                               cnt = pd.NamedAgg(column='cluster', aggfunc='count') ).\\\n",
    "                                           reset_index().sort_values('cnt', ascending=False).reset_index(drop=True)\n",
    "\n",
    "minval=results_df['cnt'].min().astype(int).item()\n",
    "maxval= 1 + results_df['cnt'].max().astype(int).item()\n",
    "# print(\"minval: {}, maxval: {}\".format(minval,maxval))\n",
    "# results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a colormap to colorcode the count of accidents in each cluster\n",
    "# It's difficult to find a good colormap to use.\n",
    "# see: https://matplotlib.org/3.3.3/gallery/color/colormap_reference.html\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "colors = cm.get_cmap('viridis', maxval)(range(maxval),bytes=True)\n",
    "\n",
    "rgbcolors = []\n",
    "for v in colors :\n",
    "    col = np.floor(v * 255)\n",
    "    r = int(col[0])\n",
    "    g = int(col[1])\n",
    "    b = int(col[2])\n",
    "    rgbcolors.append('#' + '{0:#08x}'.format(((r * 65536) + (g * 256) + b))[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the average center of each group\n",
    "latlong = results_df[['latitude','longitude']].mean(axis=0) # To center the map\n",
    "chi_map = folium.Map(location=[latlong[0], latlong[1]], zoom_start=10, width=\"90%\", height=\"90%\")\n",
    "\n",
    "# Since the dataframe is ordered, we can display the top 20 only\n",
    "for idx, coord in results_df[0:20].iterrows():\n",
    "    tooltip_content=\"Cluster: {0}, count: {1}\".format(coord['cluster'].astype(int),coord['cnt'].astype(int) )\n",
    "    folium.Circle(radius=20,\n",
    "                  location=[coord['latitude'], coord['longitude']],\n",
    "                  # popup=row.hgroup,\n",
    "                  color=rgbcolors[coord['cnt'].astype(int) ],\n",
    "                  tooltip=tooltip_content,\n",
    "                  fill=True,\n",
    "                  fill_color=rgbcolors[coord['cnt'].astype(int) ]\n",
    "    ).add_to(chi_map)\n",
    "    \n",
    "chi_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "We have identified the locations where there are at least 10 accidents based on the last 6 months of data.<br/>\n",
    "We displayed the top 20 cluster locations.\n",
    "\n",
    "We can drill down with the map to see what is around that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
